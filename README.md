# ğŸ² What's Cooking? â€” Cuisine Classification (Kaggle)
**Text Classification | TFâ€“IDF + Logistic Regression | Neural Baseline with Learned Embeddings**

![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)
![scikit-learn](https://img.shields.io/badge/scikit--learn-ML-orange)
![NLP](https://img.shields.io/badge/NLP-TF--IDF%20%7C%20Embeddings-informational)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)

---

## Overview
This project tackles Kaggleâ€™s **Whatâ€™s Cooking?** competition: predict a recipeâ€™s cuisine (20 classes) from its ingredient list. We analyze **39,774 recipes** across **20 cuisines**, run EDA, build two modeling pipelines (classical ML + neural), and compare validation and Kaggle test performance.

**Key result:** A **multinomial logistic regression** model with **TFâ€“IDF** features outperformed a neural embedding baseline and was selected as the final submission model. 

---

## Dataset
- **Source:** Kaggle â€œWhatâ€™s Cooking?â€ (recipes with ingredient lists)
- **Size:** 39,774 recipes, 20 cuisines; ~10 ingredients per recipe on average îˆ€fileciteîˆ‚turn1file0îˆ‚L17-L19îˆ  
- **Notes from EDA:** moderate class imbalance; word clouds show common ingredients (e.g., salt) vs cuisine-specific signals 


---

## Methods

### 1) TFâ€“IDF + Multinomial Logistic Regression (Final)
**Feature engineering**
- Clean text (lowercase, remove special chars, normalize spacing)
- Merge ingredients into a single string per recipe
- Vectorize using **TFâ€“IDF** with **unigrams + bigrams** to capture multi-word ingredients (e.g., â€œsoy sauceâ€) 
- Filter rare features (appear in < 3 recipes) â†’ final space: **23,593 features** 

**Modeling**
- Multinomial logistic regression with **L2 regularization** to reduce overfitting on sparse, rare features 
- Hyperparameter tuning via CV over **C âˆˆ {1, 3, 5, 7, 10}** îˆ€fileciteîˆ‚turn1file2îˆ‚L4-L6îˆ
- Tested `class_weight="balanced"`; it reduced validation accuracy by ~1 point, so the final model uses default weighting 

### 2) Neural Baseline: Learned Embeddings + Global Average Pooling
**Representation**
- Word-level tokenization with max vocab size **15,000** and padded sequence length **40**   
- Learned embeddings (not pretrained) to capture domain-specific ingredient co-occurrence patterns 

**Architecture**
- `Embedding(64) â†’ GlobalAveragePooling1D â†’ Dense(128, ReLU) â†’ Softmax`  
- 64-d embedding chosen over 128 for similar accuracy with less complexity 
- Avg pooling preferred over max pooling; alternatives (Flatten, Conv1D+MaxPool) underperformed 

---

## Experiment: preserving multi-word ingredients
We tested an alternate cleaning strategy that keeps multi-word ingredients intact by replacing spaces with underscores (e.g., `soy_sauce`). 
Result: **no meaningful improvement** for either logistic regression or the neural model. 

---

## Results
### Performance summary
| Model | Tokenization | Val Acc | Kaggle Test Acc |
|---|---|---:|---:|
| **TFâ€“IDF + Logistic Regression** | Word-level | **0.7822** | **0.7915** |
| Neural Net (Embedding + GlobalAvgPool) | Word-level | 0.7743 | 0.7754 |
| TFâ€“IDF + Logistic Regression | Ingredient-level (`soy_sauce`) | 0.7791 | 0.7874 |
| Neural Net (Embedding + GlobalAvgPool) | Ingredient-level (`soy_sauce`) | 0.7742 | 0.7754 |

**Final model:** **TFâ€“IDF + Logistic Regression (word-level)** â€” selected for best generalization and Kaggle score. îˆ€fileciteîˆ‚turn1file1îˆ‚L24-L30îˆ

---

## Figures (generated in notebook)
- **Cuisine word clouds** (EDA): highlight common vs discriminative ingredients  
- **Neural network architecture**: Embedding â†’ GlobalAvgPool â†’ Dense â†’ Softmax   

---

## How to run
```bash
# 1) Clone
git clone https://github.com/MiladEbrahimiAbyzandi/whats-cooking-cuisine-classification-nlp.git
cd whats-cooking-cuisine-classification

# 2) Create environment (example)
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 3) Launch notebook
jupyter notebook
```

---

## Authors
- **Milad Ebrahimi Abyazandi**

Course context: DATA6100 (Fall 2025).
